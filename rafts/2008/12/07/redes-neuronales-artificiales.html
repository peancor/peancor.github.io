<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Redes neuronales artificiales | la ciudadela</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Redes neuronales artificiales" />
<meta name="author" content="peancor" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A juicio de Rumelhart, el cognitivismo cl&aacute;sico centr&oacute; sus esfuerzos explicativos de la mente en el nivel l&oacute;gico o funcional, es decir, se consideraba a la mente como un programa de ordenador (software) y se supon&iacute;a que dicho programa podr&iacute;a ejecutarse en cualquier m&aacute;quina, pues se entiende que el nivel f&iacute;sico, es decir, el cerebro (wetware), se basar&aacute; en sistemas de prop&oacute;sito general de tipo von Newmann. Esto se conoce como la met&aacute;fora del ordenador. Los conexionistas, sin embargo, dirigieron su mirada al nivel de implementaci&oacute;n, el cerebro, pues consideraban que a partir de &eacute;l se podr&iacute;an dise&ntilde;ar m&aacute;s adecuadamente algoritmos que se puedan utilizar para explicar los fen&oacute;menos mentales. Por lo tanto, la met&aacute;fora del ordenador fue sustituida por la met&aacute;fora cerebral de la mente abandonando la computaci&oacute;n de estilo simb&oacute;lico por una computaci&oacute;n inspirada en el funcionamiento cerebral. Al mirar hacia la mente, lo primero que se observ&oacute;, gracias a los pioneros trabajos de Santiago Ram&oacute;n y Cajal fue un entramado de c&eacute;lulas conectadas entre s&iacute;. Estas c&eacute;lulas, las neuronas se pueden dividir funcional y anat&oacute;micamente en tres zonas: el cuerpo celular o soma, ax&oacute;n y dendritas. Cada neurona recibe informaci&oacute;n a trav&eacute;s de las dendritas, la procesa principalmente en el soma y la proyectan a trav&eacute;s del ax&oacute;n. La conexi&oacute;n entre axones y dendritas recibe el nombre de sinapsis. El siguiente video muestra un resumen de lo que son las neuronas. En 1943, dos neurocient&iacute;ficos, Warren McCulloch y Walter Pitts, propusieron un modelo de neurona b&aacute;sico siendo pioneros en el intento de definir formalmente las neuronas como elementos computacionales y en explorar las consecuencias de sus propiedades. Desde su definici&oacute;n, a pesar de los grandes avances en el campo de la neurociencia, las redes neuronales artificiales no han cambiado mucho con respecto a las que propusieron en su d&iacute;a McCulloch y Pitts. Es importante recalcar que dicho modelo no pretende ser una teor&iacute;a del funcionamiento del cerebro del cual cada d&iacute;a se descubren cosas nuevas sino m&aacute;s bien la definici&oacute;n de un modelo de computaci&oacute;n inspirado en el cerebro que permita dise&ntilde;ar sistemas artificiales inteligentes para resolver problemas concretos. Diseccionando la neurona artificial Al igual que sus homologas biol&oacute;gicas, cada neurona consta de varias entradas y de una salida. Una neurona recibe informaci&oacute;n del entorno o de otras neuronas a trav&eacute;s de las conexiones o sinapsis de sus entradas. La informaci&oacute;n recibida determinar&aacute; el estado o nivel de activaci&oacute;n de la neurona en base a una funci&oacute;n de activaci&oacute;n. El estado de una neurona puede tomar valores continuos o discretos. Las neuronas descritas por McCulloch y Pitts pod&iacute;an estar &uacute;nicamente en dos estados: activadas (1) y desactivadas (0). El estado de una neurona se transmitir&aacute; a su salida a trav&eacute;s de la funci&oacute;n de salida, que t&iacute;picamente ser&aacute; la funci&oacute;n identidad, es decir, el estado ser&aacute; igual a la salida. Formando redes neuronales De forma an&aacute;loga a como las neuronas del cerebro crean complejos circuitos, una red neuronal artificial est&aacute; formada por m&uacute;ltiples neuronas conectadas entre s&iacute;. Las conexiones entre las neuronas se definen utilizando un par&aacute;metro denominado peso sin&aacute;ptico. Dependiendo de si el peso es positivo o negativo la naturaleza de la conexi&oacute;n ser&aacute; excitante o inhibitoria. Adem&aacute;s, el valor absoluto del peso indicar&aacute; la fuerza de la conexi&oacute;n. Si tenemos una neurona activada cuya salida este conectada a varias neuronas, esta neurona propiciara la activaci&oacute;n de las neuronas con las cuales se conecta a trav&eacute;s de conexiones excitantes y dificultara la activaci&oacute;n de aquellas neuronas con las cuales se conecte a trav&eacute;s de conexiones inhibitorias. Todo ello con una fuerza proporcional al valor absoluto del peso de la conexi&oacute;n. El valor de todos los pesos de la red neuronal en un momento temporal dado es lo que va a especificar el patr&oacute;n de conectividad que ser&aacute; lo que determine como responder&aacute; la red a las se&ntilde;ales que reciba. Los procesos de aprendizaje de las redes neuronales se basan en ir ajustando el patr&oacute;n de conectividad hasta que la red se comporte de la forma deseada para un problema concreto. Existe diversas t&eacute;cnicas para ello. Algunas redes neuronales se &lsquo;entrenar&aacute;n&rsquo; para un problema concreto y posteriormente se impedir&aacute; el reajuste de su patr&oacute;n de conectividad durante su funcionamiento mientras que otras t&eacute;cnicas de aprendizaje pueden permitir a la red adaptarse modificando su patr&oacute;n de conectividad sobre la marcha. Ejemplo En el siguiente diagrama se muestran las conexiones de una red neuronal (ANN) formada por 9 neuronas: {a, b, c, d, e, f, g, h, i}. Adem&aacute;s, en cada conexi&oacute;n se muestra su peso sin&aacute;ptico, que se recuerda que indica la naturaleza y la fuerza de esa conexi&oacute;n. De forma alternativa, el diagrama anterior puede verse como una matriz de nxn, siendo n el n&uacute;mero de neuronas, que representa el patr&oacute;n de conectividad. Adem&aacute;s, podemos representar el estado o nivel de activaci&oacute;n de cada neurona como un vector columna de tama&ntilde;o n. En la figura siguiente podemos ver tanto la matriz de conexiones como el vector de estado de las neuronas. Las columnas indican el origen de las conexiones y las filas el destino. As&iacute; si miramos a la primera columna y la tercera fila tenemos que la neurona &lsquo;a&rsquo; est&aacute; conectada a la neurona &lsquo;b&rsquo; con una fuerza de &lsquo;0.4&rsquo; tal y como se pod&iacute;a ver en el grafo. Adem&aacute;s, si observamos el vector de estado vemos que inicialmente la &uacute;nica neurona activa es la &lsquo;c&rsquo; (recordemos que en el modelo que estamos considerando una neurona solo puede estar activada o desactivada). La funci&oacute;n de activaci&oacute;n utilizada es la funci&oacute;n escal&oacute;n. Para cada neurona, se calcular&aacute; el sumatorio de todos los niveles que ingresa procedentes de otras unidades. Dicho sumatorio se conocer&aacute; como el valor que ingresa la neurona y ser&aacute; suministrado a la funci&oacute;n de activaci&oacute;n. En el caso de la funci&oacute;n escal&oacute;n, la salida ser&aacute; 1 (activaci&oacute;n) en el caso de que su ingreso sea mayor que cero y 0 (inhibici&oacute;n) en caso contrario. La vista matricial de la figura anterior facilita calcular los ingresos de la funci&oacute;n de activaci&oacute;n de cada neurona ya que lo reduce a una simple multiplicaci&oacute;n matricial. Podemos simular la red neuronal multiplicando la matriz de conexiones por el vector de estado de tal forma que obtengamos un nuevo vector de estado. Si calculamos los ingresos a partir de la figura anterior, es decir, realizamos la multiplicaci&oacute;n, obtendremos el vector [0 0 0 0 0 0.3 0 0 &ndash;0.3] y al aplicar la funci&oacute;n de activaci&oacute;n a cada uno de sus valores tendremos [0 0 0 0 0 1 0 0 0]. Es decir, la red ha pasado de tener la neurona &lsquo;c&rsquo; activa a tener la neurona &lsquo;f&rsquo; activa en una iteraci&oacute;n o&nbsp; instante temporal. La aplicaci&oacute;n recurrente de este procedimiento nos permite calcular la evoluci&oacute;n de la red que en el caso que nos ocupa se muestra en la siguiente figura. Algunas redes cambiaran indefinidamente a lo largo del tiempo, sin embargo, la red anterior, alcanzar&aacute; el estado estacionario mostrado en t=6 para la mayor&iacute;a de condiciones iniciales, lo cual convierte a dicho estado en un atractor puntual para esta red. El otro atractor de la red es el estado cero en el cual todas las neuronas estan desactivadas. Cualesquiera que sean las condiciones iniciales de esta red, a medida que evoluciona, terminar&aacute; estanc&aacute;ndose en uno de sus dos estados atractores. La arquitectura de la red anterior es recurrente, y dispersa. Recurrente porque contiene bucles en las conexiones ( c-&gt;f-&gt;c ) y dispersa porque de todas las conexiones posibles (n^2=81), solo unas pocas se han realizado. Existen m&uacute;ltiples arquitecturas de redes neurales dependiendo de c&oacute;mo se estructuran y conectan las neuronas que las componen. Bibliograf&iacute;a: From Computer to Brain - Foundations of Computational Neuroscience Cognici&oacute;n humana: mente, ordenadores y neuronas" />
<meta property="og:description" content="A juicio de Rumelhart, el cognitivismo cl&aacute;sico centr&oacute; sus esfuerzos explicativos de la mente en el nivel l&oacute;gico o funcional, es decir, se consideraba a la mente como un programa de ordenador (software) y se supon&iacute;a que dicho programa podr&iacute;a ejecutarse en cualquier m&aacute;quina, pues se entiende que el nivel f&iacute;sico, es decir, el cerebro (wetware), se basar&aacute; en sistemas de prop&oacute;sito general de tipo von Newmann. Esto se conoce como la met&aacute;fora del ordenador. Los conexionistas, sin embargo, dirigieron su mirada al nivel de implementaci&oacute;n, el cerebro, pues consideraban que a partir de &eacute;l se podr&iacute;an dise&ntilde;ar m&aacute;s adecuadamente algoritmos que se puedan utilizar para explicar los fen&oacute;menos mentales. Por lo tanto, la met&aacute;fora del ordenador fue sustituida por la met&aacute;fora cerebral de la mente abandonando la computaci&oacute;n de estilo simb&oacute;lico por una computaci&oacute;n inspirada en el funcionamiento cerebral. Al mirar hacia la mente, lo primero que se observ&oacute;, gracias a los pioneros trabajos de Santiago Ram&oacute;n y Cajal fue un entramado de c&eacute;lulas conectadas entre s&iacute;. Estas c&eacute;lulas, las neuronas se pueden dividir funcional y anat&oacute;micamente en tres zonas: el cuerpo celular o soma, ax&oacute;n y dendritas. Cada neurona recibe informaci&oacute;n a trav&eacute;s de las dendritas, la procesa principalmente en el soma y la proyectan a trav&eacute;s del ax&oacute;n. La conexi&oacute;n entre axones y dendritas recibe el nombre de sinapsis. El siguiente video muestra un resumen de lo que son las neuronas. En 1943, dos neurocient&iacute;ficos, Warren McCulloch y Walter Pitts, propusieron un modelo de neurona b&aacute;sico siendo pioneros en el intento de definir formalmente las neuronas como elementos computacionales y en explorar las consecuencias de sus propiedades. Desde su definici&oacute;n, a pesar de los grandes avances en el campo de la neurociencia, las redes neuronales artificiales no han cambiado mucho con respecto a las que propusieron en su d&iacute;a McCulloch y Pitts. Es importante recalcar que dicho modelo no pretende ser una teor&iacute;a del funcionamiento del cerebro del cual cada d&iacute;a se descubren cosas nuevas sino m&aacute;s bien la definici&oacute;n de un modelo de computaci&oacute;n inspirado en el cerebro que permita dise&ntilde;ar sistemas artificiales inteligentes para resolver problemas concretos. Diseccionando la neurona artificial Al igual que sus homologas biol&oacute;gicas, cada neurona consta de varias entradas y de una salida. Una neurona recibe informaci&oacute;n del entorno o de otras neuronas a trav&eacute;s de las conexiones o sinapsis de sus entradas. La informaci&oacute;n recibida determinar&aacute; el estado o nivel de activaci&oacute;n de la neurona en base a una funci&oacute;n de activaci&oacute;n. El estado de una neurona puede tomar valores continuos o discretos. Las neuronas descritas por McCulloch y Pitts pod&iacute;an estar &uacute;nicamente en dos estados: activadas (1) y desactivadas (0). El estado de una neurona se transmitir&aacute; a su salida a trav&eacute;s de la funci&oacute;n de salida, que t&iacute;picamente ser&aacute; la funci&oacute;n identidad, es decir, el estado ser&aacute; igual a la salida. Formando redes neuronales De forma an&aacute;loga a como las neuronas del cerebro crean complejos circuitos, una red neuronal artificial est&aacute; formada por m&uacute;ltiples neuronas conectadas entre s&iacute;. Las conexiones entre las neuronas se definen utilizando un par&aacute;metro denominado peso sin&aacute;ptico. Dependiendo de si el peso es positivo o negativo la naturaleza de la conexi&oacute;n ser&aacute; excitante o inhibitoria. Adem&aacute;s, el valor absoluto del peso indicar&aacute; la fuerza de la conexi&oacute;n. Si tenemos una neurona activada cuya salida este conectada a varias neuronas, esta neurona propiciara la activaci&oacute;n de las neuronas con las cuales se conecta a trav&eacute;s de conexiones excitantes y dificultara la activaci&oacute;n de aquellas neuronas con las cuales se conecte a trav&eacute;s de conexiones inhibitorias. Todo ello con una fuerza proporcional al valor absoluto del peso de la conexi&oacute;n. El valor de todos los pesos de la red neuronal en un momento temporal dado es lo que va a especificar el patr&oacute;n de conectividad que ser&aacute; lo que determine como responder&aacute; la red a las se&ntilde;ales que reciba. Los procesos de aprendizaje de las redes neuronales se basan en ir ajustando el patr&oacute;n de conectividad hasta que la red se comporte de la forma deseada para un problema concreto. Existe diversas t&eacute;cnicas para ello. Algunas redes neuronales se &lsquo;entrenar&aacute;n&rsquo; para un problema concreto y posteriormente se impedir&aacute; el reajuste de su patr&oacute;n de conectividad durante su funcionamiento mientras que otras t&eacute;cnicas de aprendizaje pueden permitir a la red adaptarse modificando su patr&oacute;n de conectividad sobre la marcha. Ejemplo En el siguiente diagrama se muestran las conexiones de una red neuronal (ANN) formada por 9 neuronas: {a, b, c, d, e, f, g, h, i}. Adem&aacute;s, en cada conexi&oacute;n se muestra su peso sin&aacute;ptico, que se recuerda que indica la naturaleza y la fuerza de esa conexi&oacute;n. De forma alternativa, el diagrama anterior puede verse como una matriz de nxn, siendo n el n&uacute;mero de neuronas, que representa el patr&oacute;n de conectividad. Adem&aacute;s, podemos representar el estado o nivel de activaci&oacute;n de cada neurona como un vector columna de tama&ntilde;o n. En la figura siguiente podemos ver tanto la matriz de conexiones como el vector de estado de las neuronas. Las columnas indican el origen de las conexiones y las filas el destino. As&iacute; si miramos a la primera columna y la tercera fila tenemos que la neurona &lsquo;a&rsquo; est&aacute; conectada a la neurona &lsquo;b&rsquo; con una fuerza de &lsquo;0.4&rsquo; tal y como se pod&iacute;a ver en el grafo. Adem&aacute;s, si observamos el vector de estado vemos que inicialmente la &uacute;nica neurona activa es la &lsquo;c&rsquo; (recordemos que en el modelo que estamos considerando una neurona solo puede estar activada o desactivada). La funci&oacute;n de activaci&oacute;n utilizada es la funci&oacute;n escal&oacute;n. Para cada neurona, se calcular&aacute; el sumatorio de todos los niveles que ingresa procedentes de otras unidades. Dicho sumatorio se conocer&aacute; como el valor que ingresa la neurona y ser&aacute; suministrado a la funci&oacute;n de activaci&oacute;n. En el caso de la funci&oacute;n escal&oacute;n, la salida ser&aacute; 1 (activaci&oacute;n) en el caso de que su ingreso sea mayor que cero y 0 (inhibici&oacute;n) en caso contrario. La vista matricial de la figura anterior facilita calcular los ingresos de la funci&oacute;n de activaci&oacute;n de cada neurona ya que lo reduce a una simple multiplicaci&oacute;n matricial. Podemos simular la red neuronal multiplicando la matriz de conexiones por el vector de estado de tal forma que obtengamos un nuevo vector de estado. Si calculamos los ingresos a partir de la figura anterior, es decir, realizamos la multiplicaci&oacute;n, obtendremos el vector [0 0 0 0 0 0.3 0 0 &ndash;0.3] y al aplicar la funci&oacute;n de activaci&oacute;n a cada uno de sus valores tendremos [0 0 0 0 0 1 0 0 0]. Es decir, la red ha pasado de tener la neurona &lsquo;c&rsquo; activa a tener la neurona &lsquo;f&rsquo; activa en una iteraci&oacute;n o&nbsp; instante temporal. La aplicaci&oacute;n recurrente de este procedimiento nos permite calcular la evoluci&oacute;n de la red que en el caso que nos ocupa se muestra en la siguiente figura. Algunas redes cambiaran indefinidamente a lo largo del tiempo, sin embargo, la red anterior, alcanzar&aacute; el estado estacionario mostrado en t=6 para la mayor&iacute;a de condiciones iniciales, lo cual convierte a dicho estado en un atractor puntual para esta red. El otro atractor de la red es el estado cero en el cual todas las neuronas estan desactivadas. Cualesquiera que sean las condiciones iniciales de esta red, a medida que evoluciona, terminar&aacute; estanc&aacute;ndose en uno de sus dos estados atractores. La arquitectura de la red anterior es recurrente, y dispersa. Recurrente porque contiene bucles en las conexiones ( c-&gt;f-&gt;c ) y dispersa porque de todas las conexiones posibles (n^2=81), solo unas pocas se han realizado. Existen m&uacute;ltiples arquitecturas de redes neurales dependiendo de c&oacute;mo se estructuran y conectan las neuronas que las componen. Bibliograf&iacute;a: From Computer to Brain - Foundations of Computational Neuroscience Cognici&oacute;n humana: mente, ordenadores y neuronas" />
<link rel="canonical" href="http://localhost:4000/2008/12/07/redes-neuronales-artificiales.html" />
<meta property="og:url" content="http://localhost:4000/2008/12/07/redes-neuronales-artificiales.html" />
<meta property="og:site_name" content="la ciudadela" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2008-12-07T00:00:00+01:00" />
<script type="application/ld+json">
{"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2008/12/07/redes-neuronales-artificiales.html"},"author":{"@type":"Person","name":"peancor"},"@type":"BlogPosting","url":"http://localhost:4000/2008/12/07/redes-neuronales-artificiales.html","headline":"Redes neuronales artificiales","dateModified":"2008-12-07T00:00:00+01:00","datePublished":"2008-12-07T00:00:00+01:00","description":"A juicio de Rumelhart, el cognitivismo cl&aacute;sico centr&oacute; sus esfuerzos explicativos de la mente en el nivel l&oacute;gico o funcional, es decir, se consideraba a la mente como un programa de ordenador (software) y se supon&iacute;a que dicho programa podr&iacute;a ejecutarse en cualquier m&aacute;quina, pues se entiende que el nivel f&iacute;sico, es decir, el cerebro (wetware), se basar&aacute; en sistemas de prop&oacute;sito general de tipo von Newmann. Esto se conoce como la met&aacute;fora del ordenador. Los conexionistas, sin embargo, dirigieron su mirada al nivel de implementaci&oacute;n, el cerebro, pues consideraban que a partir de &eacute;l se podr&iacute;an dise&ntilde;ar m&aacute;s adecuadamente algoritmos que se puedan utilizar para explicar los fen&oacute;menos mentales. Por lo tanto, la met&aacute;fora del ordenador fue sustituida por la met&aacute;fora cerebral de la mente abandonando la computaci&oacute;n de estilo simb&oacute;lico por una computaci&oacute;n inspirada en el funcionamiento cerebral. Al mirar hacia la mente, lo primero que se observ&oacute;, gracias a los pioneros trabajos de Santiago Ram&oacute;n y Cajal fue un entramado de c&eacute;lulas conectadas entre s&iacute;. Estas c&eacute;lulas, las neuronas se pueden dividir funcional y anat&oacute;micamente en tres zonas: el cuerpo celular o soma, ax&oacute;n y dendritas. Cada neurona recibe informaci&oacute;n a trav&eacute;s de las dendritas, la procesa principalmente en el soma y la proyectan a trav&eacute;s del ax&oacute;n. La conexi&oacute;n entre axones y dendritas recibe el nombre de sinapsis. El siguiente video muestra un resumen de lo que son las neuronas. En 1943, dos neurocient&iacute;ficos, Warren McCulloch y Walter Pitts, propusieron un modelo de neurona b&aacute;sico siendo pioneros en el intento de definir formalmente las neuronas como elementos computacionales y en explorar las consecuencias de sus propiedades. Desde su definici&oacute;n, a pesar de los grandes avances en el campo de la neurociencia, las redes neuronales artificiales no han cambiado mucho con respecto a las que propusieron en su d&iacute;a McCulloch y Pitts. Es importante recalcar que dicho modelo no pretende ser una teor&iacute;a del funcionamiento del cerebro del cual cada d&iacute;a se descubren cosas nuevas sino m&aacute;s bien la definici&oacute;n de un modelo de computaci&oacute;n inspirado en el cerebro que permita dise&ntilde;ar sistemas artificiales inteligentes para resolver problemas concretos. Diseccionando la neurona artificial Al igual que sus homologas biol&oacute;gicas, cada neurona consta de varias entradas y de una salida. Una neurona recibe informaci&oacute;n del entorno o de otras neuronas a trav&eacute;s de las conexiones o sinapsis de sus entradas. La informaci&oacute;n recibida determinar&aacute; el estado o nivel de activaci&oacute;n de la neurona en base a una funci&oacute;n de activaci&oacute;n. El estado de una neurona puede tomar valores continuos o discretos. Las neuronas descritas por McCulloch y Pitts pod&iacute;an estar &uacute;nicamente en dos estados: activadas (1) y desactivadas (0). El estado de una neurona se transmitir&aacute; a su salida a trav&eacute;s de la funci&oacute;n de salida, que t&iacute;picamente ser&aacute; la funci&oacute;n identidad, es decir, el estado ser&aacute; igual a la salida. Formando redes neuronales De forma an&aacute;loga a como las neuronas del cerebro crean complejos circuitos, una red neuronal artificial est&aacute; formada por m&uacute;ltiples neuronas conectadas entre s&iacute;. Las conexiones entre las neuronas se definen utilizando un par&aacute;metro denominado peso sin&aacute;ptico. Dependiendo de si el peso es positivo o negativo la naturaleza de la conexi&oacute;n ser&aacute; excitante o inhibitoria. Adem&aacute;s, el valor absoluto del peso indicar&aacute; la fuerza de la conexi&oacute;n. Si tenemos una neurona activada cuya salida este conectada a varias neuronas, esta neurona propiciara la activaci&oacute;n de las neuronas con las cuales se conecta a trav&eacute;s de conexiones excitantes y dificultara la activaci&oacute;n de aquellas neuronas con las cuales se conecte a trav&eacute;s de conexiones inhibitorias. Todo ello con una fuerza proporcional al valor absoluto del peso de la conexi&oacute;n. El valor de todos los pesos de la red neuronal en un momento temporal dado es lo que va a especificar el patr&oacute;n de conectividad que ser&aacute; lo que determine como responder&aacute; la red a las se&ntilde;ales que reciba. Los procesos de aprendizaje de las redes neuronales se basan en ir ajustando el patr&oacute;n de conectividad hasta que la red se comporte de la forma deseada para un problema concreto. Existe diversas t&eacute;cnicas para ello. Algunas redes neuronales se &lsquo;entrenar&aacute;n&rsquo; para un problema concreto y posteriormente se impedir&aacute; el reajuste de su patr&oacute;n de conectividad durante su funcionamiento mientras que otras t&eacute;cnicas de aprendizaje pueden permitir a la red adaptarse modificando su patr&oacute;n de conectividad sobre la marcha. Ejemplo En el siguiente diagrama se muestran las conexiones de una red neuronal (ANN) formada por 9 neuronas: {a, b, c, d, e, f, g, h, i}. Adem&aacute;s, en cada conexi&oacute;n se muestra su peso sin&aacute;ptico, que se recuerda que indica la naturaleza y la fuerza de esa conexi&oacute;n. De forma alternativa, el diagrama anterior puede verse como una matriz de nxn, siendo n el n&uacute;mero de neuronas, que representa el patr&oacute;n de conectividad. Adem&aacute;s, podemos representar el estado o nivel de activaci&oacute;n de cada neurona como un vector columna de tama&ntilde;o n. En la figura siguiente podemos ver tanto la matriz de conexiones como el vector de estado de las neuronas. Las columnas indican el origen de las conexiones y las filas el destino. As&iacute; si miramos a la primera columna y la tercera fila tenemos que la neurona &lsquo;a&rsquo; est&aacute; conectada a la neurona &lsquo;b&rsquo; con una fuerza de &lsquo;0.4&rsquo; tal y como se pod&iacute;a ver en el grafo. Adem&aacute;s, si observamos el vector de estado vemos que inicialmente la &uacute;nica neurona activa es la &lsquo;c&rsquo; (recordemos que en el modelo que estamos considerando una neurona solo puede estar activada o desactivada). La funci&oacute;n de activaci&oacute;n utilizada es la funci&oacute;n escal&oacute;n. Para cada neurona, se calcular&aacute; el sumatorio de todos los niveles que ingresa procedentes de otras unidades. Dicho sumatorio se conocer&aacute; como el valor que ingresa la neurona y ser&aacute; suministrado a la funci&oacute;n de activaci&oacute;n. En el caso de la funci&oacute;n escal&oacute;n, la salida ser&aacute; 1 (activaci&oacute;n) en el caso de que su ingreso sea mayor que cero y 0 (inhibici&oacute;n) en caso contrario. La vista matricial de la figura anterior facilita calcular los ingresos de la funci&oacute;n de activaci&oacute;n de cada neurona ya que lo reduce a una simple multiplicaci&oacute;n matricial. Podemos simular la red neuronal multiplicando la matriz de conexiones por el vector de estado de tal forma que obtengamos un nuevo vector de estado. Si calculamos los ingresos a partir de la figura anterior, es decir, realizamos la multiplicaci&oacute;n, obtendremos el vector [0 0 0 0 0 0.3 0 0 &ndash;0.3] y al aplicar la funci&oacute;n de activaci&oacute;n a cada uno de sus valores tendremos [0 0 0 0 0 1 0 0 0]. Es decir, la red ha pasado de tener la neurona &lsquo;c&rsquo; activa a tener la neurona &lsquo;f&rsquo; activa en una iteraci&oacute;n o&nbsp; instante temporal. La aplicaci&oacute;n recurrente de este procedimiento nos permite calcular la evoluci&oacute;n de la red que en el caso que nos ocupa se muestra en la siguiente figura. Algunas redes cambiaran indefinidamente a lo largo del tiempo, sin embargo, la red anterior, alcanzar&aacute; el estado estacionario mostrado en t=6 para la mayor&iacute;a de condiciones iniciales, lo cual convierte a dicho estado en un atractor puntual para esta red. El otro atractor de la red es el estado cero en el cual todas las neuronas estan desactivadas. Cualesquiera que sean las condiciones iniciales de esta red, a medida que evoluciona, terminar&aacute; estanc&aacute;ndose en uno de sus dos estados atractores. La arquitectura de la red anterior es recurrente, y dispersa. Recurrente porque contiene bucles en las conexiones ( c-&gt;f-&gt;c ) y dispersa porque de todas las conexiones posibles (n^2=81), solo unas pocas se han realizado. Existen m&uacute;ltiples arquitecturas de redes neurales dependiendo de c&oacute;mo se estructuran y conectan las neuronas que las componen. Bibliograf&iacute;a: From Computer to Brain - Foundations of Computational Neuroscience Cognici&oacute;n humana: mente, ordenadores y neuronas","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="la ciudadela" /><script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
  </script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">la ciudadela</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Redes neuronales artificiales</h1>
  </header>

  <div class="post-content">
    <p>A juicio de <a href="http://en.wikipedia.org/wiki/David_Rumelhart" target="_blank">Rumelhart</a>, el cognitivismo cl&aacute;sico centr&oacute; sus esfuerzos explicativos de la mente en el nivel l&oacute;gico o funcional, es decir, se consideraba a la mente como un programa de ordenador (software) y se supon&iacute;a que dicho programa podr&iacute;a ejecutarse en cualquier m&aacute;quina, pues se entiende que el nivel f&iacute;sico, es decir, el cerebro (wetware), se basar&aacute; en sistemas de prop&oacute;sito general de tipo <a href="http://es.wikipedia.org/wiki/Arquitectura_Eckert-Mauchly" target="_blank">von Newmann</a>. Esto se conoce como la met&aacute;fora del ordenador.</p>
<p>Los conexionistas, sin embargo, dirigieron su mirada al nivel de implementaci&oacute;n, el cerebro, pues consideraban que a partir de &eacute;l se podr&iacute;an dise&ntilde;ar m&aacute;s adecuadamente algoritmos que se puedan utilizar para explicar los fen&oacute;menos mentales. Por lo tanto, la met&aacute;fora del ordenador fue sustituida por la met&aacute;fora cerebral de la mente abandonando la computaci&oacute;n de estilo simb&oacute;lico por una computaci&oacute;n inspirada en el funcionamiento cerebral.</p>
<p>Al mirar hacia la mente, lo primero que se observ&oacute;, gracias a los pioneros trabajos de <a href="http://es.wikipedia.org/wiki/Santiago_Ram%C3%B3n_y_Cajal" target="_blank">Santiago Ram&oacute;n y Cajal</a> fue un entramado de c&eacute;lulas conectadas entre s&iacute;. Estas c&eacute;lulas, las neuronas se pueden dividir funcional y anat&oacute;micamente en tres zonas: el cuerpo celular o soma, ax&oacute;n y dendritas. Cada neurona recibe informaci&oacute;n a trav&eacute;s de las dendritas, la procesa principalmente en el soma y la proyectan a trav&eacute;s del ax&oacute;n. La conexi&oacute;n entre axones y dendritas recibe el nombre de sinapsis. El siguiente video muestra un resumen de lo que son las neuronas.</p>

<div class="flex-video">
  <iframe id="ytplayer" type="text/html" width="640" height="360" src="https://youtube.com/embed/wJbyQaoYf3I" frameborder="0"></iframe>
</div>

<p>En 1943, dos neurocient&iacute;ficos, <a href="http://en.wikipedia.org/wiki/Warren_McCulloch">Warren McCulloch</a> y <a href="http://en.wikipedia.org/wiki/Walter_Pitts">Walter Pitts</a>, propusieron un modelo de neurona b&aacute;sico siendo pioneros en el intento de definir formalmente las neuronas como elementos computacionales y en explorar las consecuencias de sus propiedades. Desde su definici&oacute;n, a pesar de los grandes avances en el campo de la neurociencia, las redes neuronales artificiales no han cambiado mucho con respecto a las que propusieron en su d&iacute;a McCulloch y Pitts.</p>
<p>Es importante recalcar que dicho modelo no pretende ser una teor&iacute;a del funcionamiento del cerebro del cual cada <a href="http://www.tendencias21.net/El-proceso-cerebral-de-aprender-y-recordar-es-cosa-de-dos_a2803.html?preaction=nl&amp;id=6225296&amp;idnl=42737&amp;" target="_blank">d&iacute;a se descubren cosas nuevas</a> sino m&aacute;s bien la definici&oacute;n de un modelo de computaci&oacute;n inspirado en el cerebro que permita dise&ntilde;ar sistemas artificiales inteligentes para resolver problemas concretos.</p>
<h3>Diseccionando la neurona artificial</h3>
<p>Al igual que sus homologas biol&oacute;gicas, cada neurona consta de varias entradas y de una salida. Una neurona recibe informaci&oacute;n del entorno o de otras neuronas a trav&eacute;s de las conexiones o sinapsis de sus entradas. La informaci&oacute;n recibida determinar&aacute; el <strong>estado o nivel de activaci&oacute;n</strong> de la neurona en base a una <strong>funci&oacute;n de activaci&oacute;n</strong>.</p>
<p>El estado de una neurona puede tomar valores continuos o discretos. Las neuronas descritas por McCulloch y Pitts pod&iacute;an estar &uacute;nicamente en dos estados: activadas (1) y desactivadas (0). El estado de una neurona se transmitir&aacute; a su salida a trav&eacute;s de la <strong>funci&oacute;n de salida</strong>, que t&iacute;picamente ser&aacute; la funci&oacute;n identidad, es decir, el estado ser&aacute; igual a la salida.</p>
<h3>Formando redes neuronales</h3>
<p>De forma an&aacute;loga a como las neuronas del cerebro crean complejos circuitos, una red neuronal artificial est&aacute; formada por m&uacute;ltiples neuronas conectadas entre s&iacute;.</p>
<p>Las conexiones entre las neuronas se definen utilizando un par&aacute;metro denominado <strong>peso sin&aacute;ptico</strong>. Dependiendo de si el peso es positivo o negativo la naturaleza de la conexi&oacute;n ser&aacute; excitante o inhibitoria. Adem&aacute;s, el valor absoluto del peso indicar&aacute; la fuerza de la conexi&oacute;n.</p>
<p>Si tenemos una neurona activada cuya salida este conectada a varias neuronas, esta neurona propiciara la activaci&oacute;n de las neuronas con las cuales se conecta a trav&eacute;s de conexiones excitantes y dificultara la activaci&oacute;n de aquellas neuronas con las cuales se conecte a trav&eacute;s de conexiones inhibitorias. Todo ello con una fuerza proporcional al valor absoluto del peso de la conexi&oacute;n.</p>
<p>El valor de todos los pesos de la red neuronal en un momento temporal dado es lo que va a especificar el <strong>patr&oacute;n de conectividad</strong> que ser&aacute; lo que determine como responder&aacute; la red a las se&ntilde;ales que reciba. Los procesos de aprendizaje de las redes neuronales se basan en ir ajustando el <strong>patr&oacute;n de conectividad</strong> hasta que la red se comporte de la forma deseada para un problema concreto. Existe diversas t&eacute;cnicas para ello. Algunas redes neuronales se &lsquo;entrenar&aacute;n&rsquo; para un problema concreto y posteriormente se impedir&aacute; el reajuste de su <strong>patr&oacute;n de conectividad</strong> durante su funcionamiento mientras que otras t&eacute;cnicas de aprendizaje pueden permitir a la red adaptarse modificando su <strong>patr&oacute;n de conectividad</strong> sobre la marcha.</p>
<h3>Ejemplo</h3>
<p>En el siguiente diagrama se muestran las conexiones de una red neuronal (ANN) formada por 9 neuronas: {a, b, c, d, e, f, g, h, i}. Adem&aacute;s, en cada conexi&oacute;n se muestra su <strong>peso sin&aacute;ptico</strong>, que se recuerda que indica la naturaleza y la fuerza de esa conexi&oacute;n.</p>
<p><img src="/images/07-11/2008-12-diagrama-redes-neuronales-artificiales.png" /></p>
<p>De forma alternativa, el diagrama anterior puede verse como una matriz de nxn, siendo n el n&uacute;mero de neuronas, que representa el <strong>patr&oacute;n de conectividad</strong>. Adem&aacute;s, podemos representar el <strong>estado o nivel de activaci&oacute;n</strong> de cada neurona como un vector columna de tama&ntilde;o n. En la figura siguiente podemos ver tanto la matriz de conexiones como el vector de estado de las neuronas.</p>
<p><img src="/images/07-11/2008-12-matriz-de-conexiones.png" /></p>
<p>Las columnas indican el origen de las conexiones y las filas el destino. As&iacute; si miramos a la primera columna y la tercera fila tenemos que la neurona &lsquo;a&rsquo; est&aacute; conectada a la neurona &lsquo;b&rsquo; con una fuerza de &lsquo;0.4&rsquo; tal y como se pod&iacute;a ver en el grafo. Adem&aacute;s, si observamos el vector de estado vemos que inicialmente la &uacute;nica neurona activa es la &lsquo;c&rsquo; (recordemos que en el modelo que estamos considerando una neurona solo puede estar activada o desactivada).</p>
<p>La <strong>funci&oacute;n de activaci&oacute;n</strong> utilizada es la funci&oacute;n escal&oacute;n. Para cada neurona, se calcular&aacute; el sumatorio de todos los niveles que ingresa procedentes de otras unidades. Dicho sumatorio se conocer&aacute; como el valor que ingresa la neurona y ser&aacute; suministrado a la<strong> funci&oacute;n de activaci&oacute;n</strong>. En el caso de la funci&oacute;n escal&oacute;n, la salida ser&aacute; 1 (activaci&oacute;n) en el caso de que su ingreso sea mayor que cero y 0 (inhibici&oacute;n) en caso contrario.</p>
<p>La vista matricial de la figura anterior facilita calcular los ingresos de la <strong>funci&oacute;n de activaci&oacute;n</strong> de cada neurona ya que lo reduce a una simple multiplicaci&oacute;n matricial. Podemos simular la red neuronal multiplicando la matriz de conexiones por el vector de estado de tal forma que obtengamos un nuevo vector de estado.</p>
<p>Si calculamos los ingresos a partir de la figura anterior, es decir, realizamos la multiplicaci&oacute;n, obtendremos el vector [0 0 0 0 0 0.3 0 0 &ndash;0.3] y al aplicar la <strong>funci&oacute;n de activaci&oacute;n</strong> a cada uno de sus valores tendremos [0 0 0 0 0 1 0 0 0]. Es decir, la red ha pasado de tener la neurona &lsquo;c&rsquo; activa a tener la neurona &lsquo;f&rsquo; activa en una iteraci&oacute;n o&nbsp; instante temporal. La aplicaci&oacute;n recurrente de este procedimiento nos permite calcular la evoluci&oacute;n de la red que en el caso que nos ocupa se muestra en la siguiente figura.</p>
<p><img src="/images/07-11/2008-12-secuencia-red-neuronal-artificial.png" /></p>
<p>Algunas redes cambiaran indefinidamente a lo largo del tiempo, sin embargo, la red anterior, alcanzar&aacute; el estado estacionario mostrado en t=6 para la mayor&iacute;a de condiciones iniciales, lo cual convierte a dicho estado en un atractor puntual para esta red. El otro atractor de la red es el estado cero en el cual todas las neuronas estan desactivadas. Cualesquiera que sean las condiciones iniciales de esta red, a medida que evoluciona, terminar&aacute; estanc&aacute;ndose en uno de sus dos estados atractores.</p>
<p>La arquitectura de la red anterior es recurrente, y dispersa. Recurrente porque contiene bucles en las conexiones ( c-&gt;f-&gt;c ) y dispersa porque de todas las conexiones posibles (n^2=81), solo unas pocas se han realizado. Existen m&uacute;ltiples arquitecturas de redes neurales dependiendo de c&oacute;mo se estructuran y conectan las neuronas que las componen.</p>
<h6>Bibliograf&iacute;a:    <br />
<a href="http://www.amazon.com/Computer-Brain-William-W-Lytton/dp/0387955267" target="_blank">From Computer to Brain - Foundations of Computational Neuroscience</a>     <br />
<a href="https://www.libreriadelauned.es/esp/ficha_libro.aspx?id=4001" target="_blank">Cognici&oacute;n humana: mente, ordenadores y neuronas</a></h6>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col one-half">
      <h2 class="footer-heading">la ciudadela</h2>
        <ul class="contact-list">
          <li class="p-name">peancor</li></ul>
      </div>

      <div class="footer-col one-half">
        <p>Blog personal de Pedro Anuarbe Cortes (@peancor)
</p>
      </div>

      <div class="social-links"><ul class="social-media-list"><li><a href="https://github.com/peancor" title="peancor"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a href="https://twitter.com/peancor" title="peancor"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li><li><a href="/feed.xml" title="rss"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#rss"></use></svg></a></li></ul>
</div>
    </div>

  </div>

</footer>
</body>

</html>
